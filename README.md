
Music Generation-DONE
Introduction
Dataset-DONE
Model architecture-DONE
Results


# Music Generation Model
This repository contains a music generation model that generates original music compositions using deep learning techniques and TensorFlow

# Dataset
The music generation model is trained on the ABC notation dataset provided in the MIT Deep Learning Library(MDL).

The dataset contains 817 songs in text format. 
These songs will be used to train the model to generate new music.

# Introduction
ABC files contain information about the notes, timing, and velocity of each sound, making them well-suited for music generation using deep learning techniques.

By training the model on a dataset of existing songs, the model can learn patterns and structures in music and generate new melodies, harmonies, and rhythms. 

LSTM model can generate music that is both pleasing to the ear and follows a coherent musical structure. 



# Model Architecture
The model architecture is based on a recurrent neural network (RNN) with LSTM cells.

 The LSTM cells allow the model to capture long-term dependencies and generate coherent and melodic musical sequences.

# Training
To train the model, run the train.py script. The script will load the preprocessed dataset, train the model using TensorFlow, and save the trained weights for future use.

# Generation
To generate new music using the trained model, run the generate.py script. Specify the desired length of the generated sequence and the output file path. The script will utilize the trained model to create an original composition.

# Results
Some sample compositions generated by the model have been included in the results directory.
Feel free to listen to them and explore the creative capabilities of the system.


create a generate.py

![WAV File](results/song_1.wav)

<audio controls>
  <source src="results/song_1.wav" type="audio/wav">
  Your browser does not support the audio element.
</audio>
